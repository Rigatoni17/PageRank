{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Basic Data Science Projects using Python, NumPy, Pandas, Matplotlib, Regular Expressions, and SQL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic 3: PageRank (np, matplotlib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank vertices in directed networks according to their importance as a function of their incoming connections. [Star Wars social networks](https://github.com/evelinag/star-wars-network-data/tree/1.0.1), Kaggle's [Trade Network](https://www.kaggle.com/datasets/yasirtariq/tradenetwork/data), [World Bank data](https://www.worldbank.org/ext/en/home)\n",
    "\n",
    "PageRank is a method developed by Google's founders, Larry Page and Sergey Brin, to rank web pages in search engine results. The core idea is that a web page's importance is determined by the number and significance of other pages that link to it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will analyze a Star Wars character interaction network, where:\n",
    "- Each node represents a character.\n",
    "- An edge exists between two nodes if the characters interact by speaking to each other in the same scene.\n",
    "\n",
    "We will apply the PageRank algorithm to rank the characters based on their importance in the network.\n",
    "\n",
    "The data is available at [Star Wars social networks](https://github.com/evelinag/star-wars-network-data/tree/1.0.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_data_1(file_path):\n",
    "    \"\"\"\n",
    "    Load the JSON file and return the characters and interactions\n",
    "    IN: file_path, str, path to the JSON files\n",
    "    OUT: list of dict, characters\n",
    "         list of tuple, interactions\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # extract dictionaries\n",
    "    characters = [\n",
    "        {\n",
    "            \"name\": val[\"name\"],\n",
    "            \"value\": val[\"value\"],\n",
    "            \"color\": val[\"colour\"],\n",
    "            \"index\": index\n",
    "        }\n",
    "        for index, val in enumerate(data[\"nodes\"])\n",
    "    ]\n",
    "\n",
    "    # extract interactions\n",
    "    interactions = [(interaction[\"source\"], interaction[\"target\"], interaction[\"value\"]) for interaction in data[\"links\"]]\n",
    "\n",
    "    return characters, interactions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convert_to_2D_array_1(interactions, characters):\n",
    "    \"\"\"\n",
    "    Convert the list of triples to a 2D NumPy array\n",
    "    IN: interactions, list of tuple, interactions\n",
    "        characters, list of dict, characters\n",
    "    OUT: ndarray of shape (n, n), 2D array of interactions\n",
    "    \"\"\"\n",
    "    n = len(characters)\n",
    "    M = np.zeros((n, n), dtype = int)\n",
    "\n",
    "    for source, target, value in interactions:\n",
    "        M[source, target] += value\n",
    "        M[target, source] += value\n",
    "    \n",
    "    for character in characters:\n",
    "        index = character[\"index\"]\n",
    "        M[index, index] = character[\"value\"]\n",
    "    \n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_unweighted_1(M):\n",
    "    \"\"\"\n",
    "    Convert the 2D NumPy array to an unweighted 2D NumPy array\n",
    "    IN: M, ndarray of shape (n, n), 2D array of interactions\n",
    "    OUT: ndarray of shape (n, n), unweighted 2D array of interactions\n",
    "    \"\"\"\n",
    "    M_unweighted = (M > 0).astype(int) # 1 if M[i, j] > 0, 0 otherwise\n",
    "    np.fill_diagonal(M_unweighted, 0) # diagonal is 0\n",
    "\n",
    "    return M_unweighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_array_1(M_unweighted):\n",
    "    \"\"\"\n",
    "    Normalize the 2D NumPy array\n",
    "    IN: M_unweighted, ndarray of shape (n, n), unweighted 2D array of interactions\n",
    "    OUT: ndarray of shape (n, n), normalized 2D array of interactions\n",
    "    \"\"\"\n",
    "    # calculate sum of each column\n",
    "    column_sums = M_unweighted.sum(axis = 0)\n",
    "\n",
    "    # avoid division by 0\n",
    "    column_sums[column_sums == 0] = 1\n",
    "\n",
    "    M_normalized = M_unweighted / column_sums\n",
    "\n",
    "    return M_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_pagerank_1(n):\n",
    "    \"\"\"\n",
    "    Initialize the PageRank vector\n",
    "    IN: n, int, number of characters\n",
    "    OUT: ndarray of shape (n, 1), PageRank vector\n",
    "    \"\"\"\n",
    "    return np.full((n, 1), 1 / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pagerank_1(r, M_normalized, alpha=0.85):\n",
    "    \"\"\"\n",
    "    Update the PageRank vector\n",
    "    IN: r, ndarray of shape (n, 1), PageRank vector\n",
    "        M_normalized, ndarray of shape (n, n), normalized 2D array of interactions\n",
    "        alpha, float, damping factor\n",
    "    OUT: ndarray of shape (n, 1), updated PageRank vector\n",
    "    \"\"\"\n",
    "    n = len(r)\n",
    "    new_r = (alpha * np.matmul(M_normalized, r)) + ((1 - alpha) * (np.ones((n, 1)) / n))\n",
    "\n",
    "    return new_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_difference_1(r1, r2):\n",
    "    \"\"\"\n",
    "    Calculate the difference between two PageRank vectors\n",
    "    IN: r1, ndarray of shape (n, 1), PageRank vector\n",
    "        r2, ndarray of shape (n, 1), PageRank vector\n",
    "    OUT: float, difference between two PageRank vectors\n",
    "    \"\"\"\n",
    "    distance = np.linalg.norm(r1 - r2)\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pagerank_1(M_normalized, alpha=0.85, threshold=1e-6, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Calculate the PageRank vector\n",
    "    IN: M_normalized, ndarray of shape (n, n), normalized 2D array of interactions\n",
    "        alpha, float, damping factor\n",
    "        threshold, float, threshold for the difference between the current and previous PageRank vectors\n",
    "        max_iter, int, maximum number of iterations\n",
    "    OUT: ndarray of shape (n, 1), PageRank vector\n",
    "    \"\"\"\n",
    "    # initialize PageRank vector\n",
    "    n = M_normalized.shape[0]\n",
    "    curr = initialize_pagerank_1(n)\n",
    "\n",
    "    # initialize previous vector\n",
    "    prev = np.zeros_like(curr)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        curr = update_pagerank_1(curr, M_normalized, alpha) # update PageRank vector\n",
    "        difference = get_difference_1(curr, prev)\n",
    "\n",
    "        # if difference is less than specified threshold, break and return current vector\n",
    "        if difference < threshold:\n",
    "            break\n",
    "\n",
    "        prev = curr.copy()\n",
    "    \n",
    "    return curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_nodes_1(r, characters, k=1):\n",
    "    \"\"\"\n",
    "    Return the top k nodes with the highest PageRank values\n",
    "    IN: r, ndarray of shape (n, 1), PageRank vector\n",
    "        characters, list of dict, characters\n",
    "        k, int, number of top nodes\n",
    "    OUT: list of dict, top k nodes\n",
    "    \"\"\"\n",
    "    # flatten to 1D array\n",
    "    r_flat = r.flatten()\n",
    "\n",
    "    # get top k indices with highest values (descending order)\n",
    "    top_indices = np.argsort(r_flat)[-k:][::-1]\n",
    "\n",
    "    # get top k nodes using top k indices\n",
    "    top_k_nodes = [characters[i] for i in top_indices]\n",
    "\n",
    "    return top_k_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Brin, S.; Page, L. (1998). \"The anatomy of a large-scale hypertextual Web search engine\" (PDF). Computer Networks and ISDN Systems. 30 (1–7): 107–117. CiteSeerX 10.1.1.115.5930. doi:10.1016/S0169-7552(98)00110-X. ISSN 0169-7552. S2CID 7587743."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
